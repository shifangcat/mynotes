import{_ as n}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as s,c as e,f as o}from"./app-VKLC1A-x.js";const t={},a=o(`<h1 id="特征工程" tabindex="-1"><a class="header-anchor" href="#特征工程" aria-hidden="true">#</a> 特征工程</h1><h2 id="特征工程-1" tabindex="-1"><a class="header-anchor" href="#特征工程-1" aria-hidden="true">#</a> 特征工程</h2><p>常用数据集数据的结构组成：特征值+目标值</p><p><em><strong>注</strong></em>：有些数据集可以没有目标值</p><p>机器学习中的重复值不需要去重</p><p><em><strong>特征工程</strong></em>：特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。特征工程能够直接影响模型的预测结果</p><p><strong>常用数据来源</strong>：</p><p>Kaggle网址：https://www.kaggle.com/datasets</p><p>UCI数据集网址： http://archive.ics.uci.edu/ml/</p><p>scikit-learn网址：http://scikit-learn.org/stable/</p><h3 id="scikit-learn库介绍" tabindex="-1"><a class="header-anchor" href="#scikit-learn库介绍" aria-hidden="true">#</a> Scikit-learn库介绍</h3><p>Scikit-learn库是Python语言的机器学习工具，包括许多知名的机器学习算法，文档完善，容易上手，具有丰富的API接口</p><p><strong>安装</strong>：<code>pip install Scikit-learn</code></p><p><strong>使用导入</strong>：<code>import sklearn</code></p><p><em><strong>注</strong></em>：安装scikit-learn需要Numpy、pandas等库</p><p>官方网站：https://scikit-learn.org/stable/</p><p>中文文档：https://www.sklearncn.cn/</p><h2 id="特征抽取" tabindex="-1"><a class="header-anchor" href="#特征抽取" aria-hidden="true">#</a> 特征抽取</h2><p>特征抽取针对非连续型数据</p><p>特征抽取对文本等进行特征值化</p><p><em><strong>注</strong></em>：特征值化是为了计算机更好地去理解数据</p><p><strong>sklearn特征抽取的API</strong>：<code>sklearn.feature_extraction</code></p><h3 id="字典特征抽取" tabindex="-1"><a class="header-anchor" href="#字典特征抽取" aria-hidden="true">#</a> 字典特征抽取</h3><p><strong>作用</strong>：对字典数据进行特征值化</p><ul><li><p><strong>字典特征抽取的API</strong></p><p><code>sklearn.feature_extraction.DictVectorizer(sparse=True, ...)</code></p><ul><li><p><strong>常用的方法</strong>：</p><ul><li><p><code>DictVectorizer.fit_transform(X)</code></p><blockquote><p><code>X</code>：字典或者包含字典的迭代器</p><p><strong>返回值</strong>：返回sparse矩阵</p></blockquote></li><li><p><code>DictVectorizer.inverse_transform(X)</code></p><blockquote><p><code>X</code>：array数组或者sparse矩阵</p><p><strong>返回值</strong>：转换之前数据格式</p></blockquote></li><li><p><code>DictVectorizer.get_feature_names()</code></p><blockquote><p><strong>返回值</strong>：返回类别名称</p></blockquote></li><li><p><code>DictVectorizer.transform(X)</code></p><blockquote><p>按照<code>fit_transform()</code>方法中得到的数据对应关系进行数据转换</p></blockquote></li></ul></li></ul></li></ul><h3 id="文本特征抽取" tabindex="-1"><a class="header-anchor" href="#文本特征抽取" aria-hidden="true">#</a> 文本特征抽取</h3><p><strong>作用</strong>：对文本数据进行特征值化</p><ul><li><p><strong>文本特征抽取的API</strong></p><p><code>sklearn.feature_extraction.text.CountVectorizer(max_df=1.0, min_df=1, ...)</code></p><blockquote><p>返回词频矩阵</p></blockquote><ul><li><p><strong>常用的方法</strong>：</p><ul><li><p><code>CountVectorizer.fit_transform(X, y)</code></p><blockquote><p><code>X</code>：文本或者包含文本字符串的可迭代对象</p><p><strong>返回值</strong>：返回sparse矩阵，使用<code>toarray()</code>方法可以将sparse矩阵转换为array数组</p></blockquote></li><li><p><code>CountVectorizer.inverse_transform(X)</code></p><blockquote><p><code>X</code>：array数组或者sparse矩阵</p><p><strong>返回值</strong>：反向转换为转换之前的原始数据</p></blockquote></li><li><p><code>CountVectorizer.get_feature_names()</code></p><blockquote><p><strong>返回值</strong>：所有文章中出现过的所有单词组成的列表，每个单词只统计一次，且单个字母不统计</p></blockquote></li></ul></li></ul><p>对每篇文章，在词的列表里面统计每个词出现的次数</p></li></ul><h3 id="中文文本特征抽取" tabindex="-1"><a class="header-anchor" href="#中文文本特征抽取" aria-hidden="true">#</a> 中文文本特征抽取</h3><p>默认不支持中文文本特征抽取，需要借助jieba分词进行操作</p><p><strong>安装</strong>：<code>pip install jieba</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer
<span class="token keyword">import</span> jieba

<span class="token keyword">def</span> <span class="token function">cutword</span><span class="token punctuation">(</span>word_li<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    cutword_li <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> word <span class="token keyword">in</span> word_li<span class="token punctuation">:</span>
        cutword <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        content <span class="token operator">=</span> <span class="token string">&#39; &#39;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>cutword<span class="token punctuation">)</span><span class="token punctuation">)</span>
        cutword_li<span class="token punctuation">.</span>append<span class="token punctuation">(</span>content<span class="token punctuation">)</span>
    <span class="token keyword">return</span> cutword_li

<span class="token keyword">def</span> <span class="token function">zhcountvec</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    zh_s <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string">&#39;今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上。&#39;</span><span class="token punctuation">,</span>
        <span class="token string">&#39;我们看到的从很远星系来的光是在几百万年之前发出的，我们是在看它的过去。&#39;</span><span class="token punctuation">,</span>
        <span class="token string">&#39;如果只用一种方式了解某样事物，你就不会真正了解它。&#39;</span>
    <span class="token punctuation">]</span>
    zh_s <span class="token operator">=</span> cutword<span class="token punctuation">(</span>zh_s<span class="token punctuation">)</span>
    cv <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    ret <span class="token operator">=</span> cv<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>zh_s<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>cv<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>ret<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 将sparse矩阵转换为array数组</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="tf-idf文本特征抽取" tabindex="-1"><a class="header-anchor" href="#tf-idf文本特征抽取" aria-hidden="true">#</a> TF-IDF文本特征抽取</h3><p><strong>TF-IDF的主要思想是</strong>：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为该词或者该短语具有很好的类别区分能力，适合用来分类</p><p><strong>TF-IDF作用</strong>：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度</p><p><strong>计算原理</strong>：</p><p>TF（term frequency）：该词出现的频率</p><p>IDF（inverse document frequency）：逆文档频率，通过<code>log(总文档数量/出现该词的文档数量)</code>来计算，log中的数值越小，结果也越小</p><p><code>该词的重要性 = TF * IDF</code></p><ul><li><p><strong>TF-IDF的API</strong></p><p><code>sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None, ...)</code></p><blockquote><p>返回词的权重矩阵</p></blockquote><ul><li><p><strong>常用的方法</strong>：</p><ul><li><p><code>TfidfVectorizer.fit_transform(X, y)</code></p><blockquote><p><code>X</code>：文本或者包含文本字符串的可迭代对象</p><p><strong>返回值</strong>：返回sparse矩阵</p></blockquote></li><li><p><code>TfidfVectorizer.inverse_transform(X)</code></p><blockquote><p><code>X</code>：array数组或者sparse矩阵</p><p><strong>返回值</strong>：反向转换为转换之前的原始数据</p></blockquote></li><li><p><code>TfidfVectorizer.get_feature_names()</code></p><blockquote><p><strong>返回值</strong>：所有文章中出现过的所有单词组成的列表，每个单词只统计一次，且单个字母不统计</p></blockquote></li></ul></li></ul></li></ul><h2 id="特征预处理" tabindex="-1"><a class="header-anchor" href="#特征预处理" aria-hidden="true">#</a> 特征预处理</h2><p><strong>特征处理</strong>：通过特定的统计方法（数学方法）将数据转换成算法要求的数据</p><p><strong>sklearn特征处理的API</strong>：<code>sklearn.preprocessing</code></p><ul><li><p><strong>特征处理的方法</strong></p><ul><li><p><em><strong>数值型数据</strong></em>：标准缩放：</p><p>1、归一化</p><p>2、标准化</p><p>3、缺失值处理</p></li><li><p><em><strong>类别型数据</strong></em>：one-hot编码</p></li><li><p><em><strong>时间类型</strong></em>：时间的切分</p></li></ul></li></ul><h3 id="归一化" tabindex="-1"><a class="header-anchor" href="#归一化" aria-hidden="true">#</a> 归一化</h3><p><strong>特点</strong>：通过对原始数据进行变换把数据映射到（默认为[0,1]）之间</p><p><strong>公式</strong>：<code>X′ = (x − min) / (max − min)</code> <code>X′′ = X′ ∗ (mx − mi) + mi</code></p><blockquote><p>说明：归一化的计算会作用于每一列，max为每列的最大值，min为每列的最小值，mx、mi分别为指定区间值的最大值和最小值，默认mx为1，mi为0，X′′为最终结果</p></blockquote><ul><li><p><strong>归一化的API</strong></p><p><code>sklearn.preprocessing.MinMaxScalar(feature_range=(0, 1), ...)</code></p><blockquote><p>每个特征缩放到给定的范围（默认[0,1]）</p></blockquote><ul><li><p><strong>常用的方法</strong>：</p><ul><li><p><code>MinMaxScalar.fit_transform(X)</code></p><blockquote><p><code>X</code>：numpy array格式的数据[n_samples, n_features]</p><p><strong>返回值</strong>：转换后形状相同的array</p></blockquote></li></ul></li></ul></li></ul><p><strong>总结</strong>：注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常<em><strong>容易受异常点影响</strong></em>，所以这种方法<em><strong>鲁棒性（稳定性）较差</strong></em>，只<em><strong>适合传统精确小数据场景</strong></em></p><h3 id="标准化" tabindex="-1"><a class="header-anchor" href="#标准化" aria-hidden="true">#</a> 标准化</h3><p><strong>特点</strong>：通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内</p><p><strong>公式</strong>：<code>X′ = x − mean / σ</code></p><blockquote><p>说明：标准化的计算会作用于每一列，mean为每列的平均值，σ为标准差（考量数据的稳定性），X′为最终结果</p></blockquote><ul><li><p><strong>标准化的API</strong></p><p><code>sklearn.preprocessing.StandardScaler(...)</code></p><blockquote><p>处理之后的每列数据都聚集在均值为0，标准差为1的附近</p></blockquote><ul><li><p><strong>常用的方法和属性</strong>：</p><ul><li><p><code>StandardScaler.fit_transform(X, y)</code></p><blockquote><p><code>X</code>：numpy array格式的数据[n_samples,n_features]</p><p><strong>返回值</strong>：转换后形状相同的array</p></blockquote></li><li><p><code>StandardScaler.mean_</code>：原始数据中每列特征的平均值</p></li></ul></li></ul></li></ul><p><strong>总结</strong>：在已有<em><strong>样本足够多</strong></em>的情况下比较稳定，<em><strong>适合现代嘈杂大数据场景</strong></em></p><p><strong>归一化和标准化的区别</strong>：</p><p>对于<em><strong>归一化</strong></em>来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变</p><p>对于<em><strong>标准化</strong></em>来说：如果出现异常点，由于具有一定的数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小</p><h3 id="缺失值处理" tabindex="-1"><a class="header-anchor" href="#缺失值处理" aria-hidden="true">#</a> 缺失值处理</h3><p><strong>缺失值的处理方法</strong>：</p><table><thead><tr><th>删除</th><th>如果每列或者每行数据缺失值达到一定的比例，建议放弃整行或者整列</th></tr></thead><tbody><tr><td>插补</td><td>可以使用每行或者每列的平均值、中位数来填充缺失值</td></tr></tbody></table><ul><li><p><strong>缺失值处理的API</strong></p><p><code>sklearn.impute.SimpleImputer(missing_values=np.nan, strategy=&quot;mean&quot;, verbose=0)</code></p><blockquote><p>完成缺失值插补</p></blockquote><ul><li><p><strong>常用的方法</strong>：</p><ul><li><p><code>SimpleImputer.fit_transform(X, y)</code></p><blockquote><p><code>X</code>：numpy array格式的数据[n_samples,n_features]</p><p><strong>返回值</strong>：转换后形状相同的array</p></blockquote></li></ul></li></ul></li></ul><h2 id="数据降维" tabindex="-1"><a class="header-anchor" href="#数据降维" aria-hidden="true">#</a> 数据降维</h2><p><strong>降维</strong>：指的是减少数据中特征的数量</p><h3 id="特征选择" tabindex="-1"><a class="header-anchor" href="#特征选择" aria-hidden="true">#</a> 特征选择</h3><p><strong>特征选择</strong>：就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也可以不改变值，选择后的特征维数肯定比选择前小，毕竟只选择了其中的一部分特征</p><p><strong>原因</strong>：</p><p><em><strong>冗余</strong></em>：部分特征的相关度高，容易消耗计算性能</p><p><em><strong>噪声</strong></em>：部分特征对预测结果有负影响</p><p><strong>主要方法</strong>：</p><p>Filter（过滤式）：VarianceThreshold</p><p>Embedded（嵌入式）：正则化、决策树</p><p>Wrapper（包裹式）</p><ul><li><p><strong>特征选择的API</strong></p><p><code>sklearn.feature_selection.VarianceThreshold(threshold=0.0)</code></p><blockquote><p>删除低方差特征，threshold指定方差大小</p></blockquote><ul><li><p><strong>常用的方法</strong>：</p><ul><li><p><code>VarianceThreshold.fit_transform(X, y)</code></p><blockquote><p><code>X</code>：numpy array格式的数据[n_samples,n_features]</p><p><strong>返回值</strong>：训练集差异低于threshold的特征将被删除</p><p>默认值是保留所有非零方差特征，即删除所有样本 中具有相同值的特征</p></blockquote></li></ul></li></ul></li></ul><h3 id="主成分分析-pca" tabindex="-1"><a class="header-anchor" href="#主成分分析-pca" aria-hidden="true">#</a> 主成分分析(PCA)</h3><p><strong>本质</strong>：PCA是一种分析、简化数据集的技术</p><p><strong>目的</strong>：使数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息</p><p><strong>作用</strong>：可以削减回归分析或者聚类分析中特征的数量</p><p><strong>高维度数据容易出现的问题</strong>：特征之间通常是线性相关的</p><ul><li><p><strong>主成分分析的API</strong></p><p><code>sklearn.decomposition.PCA(n_components=None)</code></p><blockquote><p>将数据分解为较低维度的数据</p><p><code>n_components</code>为0<sub>1之间的小数时表示保留的信息量，一般设置为90%</sub>95%</p><p><code>n_components</code>为整数时表示保留的特征数量</p></blockquote><ul><li><p><strong>常用的方法</strong>：</p><ul><li><p><code>PCA.fit_transform(X) </code></p><blockquote><p><code>X</code>：numpy array格式的数据[n_samples,n_features]</p><p><strong>返回值</strong>：转换后指定维度的array</p></blockquote></li></ul></li></ul></li></ul>`,81),p=[a];function r(c,l){return s(),e("div",null,p)}const u=n(t,[["render",r],["__file","01_特征工程.html.vue"]]);export{u as default};
