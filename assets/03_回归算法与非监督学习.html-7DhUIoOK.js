import{_ as e}from"./plugin-vue_export-helper-x3n3nnut.js";import{o,c as t,f as r}from"./app-VKLC1A-x.js";const s={},n=r('<h1 id="回归算法与非监督学习" tabindex="-1"><a class="header-anchor" href="#回归算法与非监督学习" aria-hidden="true">#</a> 回归算法与非监督学习</h1><h2 id="线性回归" tabindex="-1"><a class="header-anchor" href="#线性回归" aria-hidden="true">#</a> 线性回归</h2><h3 id="线性回归-1" tabindex="-1"><a class="header-anchor" href="#线性回归-1" aria-hidden="true">#</a> 线性回归</h3><p><strong>线性回归</strong>：通过一个或者多个自变量（特征值）与因变量（目标值）之间的关系进行建模的回归分析</p><p>一元线性回归：涉及到的变量只有一个</p><p>多元线性回归：涉及到的变量为两个或两个以上</p><p><strong>通用公式</strong>：<code>f(x) = w1x1 + w2x2 + … + wdxd + b</code></p><blockquote><p>w为权重；b称为偏置项，可以理解为：w0 * 1</p></blockquote><p>预测结果与真实值之间是有一定的误差的</p><p><em><strong>注</strong></em>：线性回归在使用前需要对数据进行<em><strong>标准化处理</strong></em></p><h3 id="损失函数-误差大小" tabindex="-1"><a class="header-anchor" href="#损失函数-误差大小" aria-hidden="true">#</a> 损失函数(误差大小)</h3><p>总损失：<code>J(θ) = (hw(x1) − y1)^2 + (hw(x2) − y2)^2 + … + (hw(xm) − ym)^ = ∑(hw(xi) − yi)^2</code></p><blockquote><p>yi为第i个训练样本的真实值；hw(xi)为第i个训练样本的预测值</p></blockquote><p>即总损失为所有真实值与预测值之间差的平方和，又称最小二乘法</p><ul><li><p><strong>最小二乘法之正规方程</strong></p><p>求解：<code>w = (X^T * X)^(−1) * X^T * y</code></p><blockquote><p>X为特征值矩阵，y为目标值矩阵，T表示转置</p></blockquote><p><strong>缺点</strong>：当特征过于复杂，求解速度太慢；对于复杂的算法，不能使用正规方程求解(逻辑回归等)</p><ul><li><p><strong>正规方程的API</strong></p><p><code>sklearn.linear_model.LinearRegression()</code></p><ul><li><strong>属性</strong>：<code>coef_</code>：回归系数</li></ul></li></ul></li><li><p><strong>最小二乘法之梯度下降</strong></p><p>理解：沿着损失函数下降的方向找，最后就能找到损失的最低点，然后 更新W值</p><p>使用：用于训练数据规模十分庞大的任务</p><ul><li><p><strong>梯度下降的API</strong></p><p><code>sklearn.linear_model.SGDRegressor()</code></p><ul><li><strong>属性</strong>：<code>coef_</code>：回归系数</li></ul></li></ul></li></ul><h3 id="回归性能评估" tabindex="-1"><a class="header-anchor" href="#回归性能评估" aria-hidden="true">#</a> 回归性能评估</h3><p>均方误差（MSE：Mean Squared Error）评价机制：</p><p><code>MSE = ∑(yi - y)^2 / m</code></p><blockquote><p>yi为预测值，y为真实值，m为总数量。即所有预测值与真实值差的平方和的平均值</p></blockquote><ul><li><p><strong>回归评估的API</strong></p><p><code>sklearn.metrics.mean_squared_error(y_true, y_pred)</code></p><blockquote><p><code>y_true</code>：真实值</p><p><code>y_pred</code>：预测值</p><p><strong>返回值</strong>：均方误差，浮点数</p><p><em><strong>注</strong></em>：真实值、预测值都是标准化之前的值</p></blockquote></li><li><p><strong>梯度下降与正规方程的比较</strong></p><table><thead><tr><th><strong>梯度下降</strong></th><th><strong>正规方程</strong></th></tr></thead><tbody><tr><td>需要选择学习率α</td><td>不需要</td></tr><tr><td>需要多次迭代</td><td>一次运算即可得出结果</td></tr><tr><td>当特征数量大时也能较好的适用</td><td>特征数量大时需要的运算代价比较大</td></tr><tr><td>适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等其他模型</td></tr></tbody></table></li></ul><h2 id="过拟合与欠拟合" tabindex="-1"><a class="header-anchor" href="#过拟合与欠拟合" aria-hidden="true">#</a> 过拟合与欠拟合</h2><p><strong>过拟合</strong>：一个模型在训练数据上能够获得很好的拟合， 但是在训练数据外的数据集上却不能很好地拟合数据，此时认为这个模型出现了过拟合的现象（模型过于复杂）</p><p><strong>欠拟合</strong>：一个模型在训练数据上不能获得很好的拟合， 同时在训练数据外的数据集上也不能很好地拟合数据，此时认为这个模型出现了欠拟合的现象（模型过于简单）</p><ul><li><p><strong>欠拟合的原因及解决办法</strong></p><ul><li><p><strong>原因</strong>：学习到的特征过少</p></li><li><p><strong>解决办法</strong>：增加数据的特征数量</p></li></ul></li><li><p><strong>过拟合的原因及解决办法</strong></p><ul><li><p><strong>原因</strong>：原始特征过多，存在一些嘈杂特征，模型过于复杂是因为模型尝试去兼顾各个测试数据点</p></li><li><p><strong>解决办法</strong>：</p><p>进行特征选择，消除关联性大的特征（很难做）</p><p>交叉验证（让所有数据都有过训练）</p><p>正则化</p></li></ul></li></ul><h3 id="l2正则化" tabindex="-1"><a class="header-anchor" href="#l2正则化" aria-hidden="true">#</a> L2正则化</h3><p><strong>作用</strong>：可以使得W的每个元素都很小，都接近于0，以解决过拟合</p><p><strong>优点</strong>：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</p><h2 id="岭回归" tabindex="-1"><a class="header-anchor" href="#岭回归" aria-hidden="true">#</a> 岭回归</h2><p><strong>岭回归</strong>：带有正则化的线性回归</p><ul><li><p><strong>岭回归的API</strong></p><p><code>sklearn.linear_model.Ridge(alpha=1.0)</code></p><blockquote><p>具有L2正则化的线性最小二乘法</p><p><code>alpha</code>：正则化力度，也有的用<code>λ</code>，取值范围一般在0<sub>1或1</sub>10</p></blockquote><ul><li><strong>属性</strong>：<code>coef_</code>：回归系数</li></ul></li><li><p><strong>岭回归的优点</strong></p><p>回归得到的回归系数更符合实际，更可靠。另外，能让估计参数的波动范围变小，变的更稳定。在异常数据偏多的研究中有较大的实用价值</p></li></ul><h2 id="模型的保存与加载" tabindex="-1"><a class="header-anchor" href="#模型的保存与加载" aria-hidden="true">#</a> 模型的保存与加载</h2><p>模型的保存与加载需要导入<code>joblib</code>包</p><ul><li><p><strong>joblib语法</strong></p><p><strong>保存</strong>：<code> joblib.dump(estimator, &#39;test.pkl&#39;)</code></p><p><strong>加载</strong>：<code> estimator = joblib.load(&#39;test.pkl&#39;)</code></p><p><em><strong>注</strong></em>：文件格式为<code>pkl</code>，是一种二进制文件</p></li></ul><h2 id="逻辑回归" tabindex="-1"><a class="header-anchor" href="#逻辑回归" aria-hidden="true">#</a> 逻辑回归</h2><p><strong>逻辑回归</strong>是一个分类算法，是解决二分类问题的利器，如：广告是否会被点击、判断性别、预测用户是否会购买指定商品、垃圾邮件检测、是否患病等</p><p><strong>sigmoid函数</strong>：输出值的范围在[0, 1]之间，默认以0.5作为阈值</p><ul><li><p><strong>逻辑回归的API</strong></p><p><code>sklearn.linear_model.LogisticRegression(penalty=&#39;l2&#39;, C=1.0)</code></p><ul><li><strong>属性</strong>：<code>coef_</code>：回归系数</li></ul></li><li><p><strong>总结</strong></p><p><strong>应用</strong>：适用于二分类问题</p><p><strong>优点</strong>：适合需要得到一个分类概率的场景，简单、速度快</p><p><strong>缺点</strong>：不能很好地处理多分类的问题</p><p><em><strong>注</strong></em>：softmax方法-逻辑回归在多分类问题上的推广（将在后面的神经网络算法中介绍）</p></li></ul><h2 id="非监督学习" tabindex="-1"><a class="header-anchor" href="#非监督学习" aria-hidden="true">#</a> 非监督学习</h2><p>非监督学习只有特征值，没有目标值。在算法学习时会进行聚类操作，即将特征相近的一些数据划分到同一个类别，但是并不知道类别的名称，常用方法为k-means算法</p><h3 id="k-means" tabindex="-1"><a class="header-anchor" href="#k-means" aria-hidden="true">#</a> k-means</h3><ul><li><p><strong>k-means算法的步骤</strong></p><p><em><strong>1&gt;</strong></em> <em><strong>随机</strong></em>设置特征空间内的k个点作为初始的聚类中心</p><p><em><strong>2&gt;</strong></em> 对于其余的每个点分别计算其到k个中心的距离，然后<em><strong>选择距离最近的一个聚类中心作为其标记类别</strong></em></p><p><em><strong>3&gt;</strong></em> 分别计算这k个族群的<em><strong>平均值</strong></em>作为新的聚类中心</p><p><em><strong>4&gt;</strong></em> 如果计算得出的新聚类中心与原聚类中心相同，则聚类结束；否则重新进行第二步和第三步的过程，直至得到的<em><strong>新旧聚类中心相同</strong></em></p></li><li><p><strong>k-means算法的API</strong></p><p><code>sklearn.cluster.KMeans(n_clusters=8, init=&#39;k-means++&#39;)</code></p><blockquote><p><code>n_clusters</code>：聚类中心的数量</p><p><code>init</code>：初始化方法，默认为&#39;k-means ++&#39;</p></blockquote><ul><li><strong>属性</strong>：<code>labels_</code>：默认标记的类型，可以和真实值比较（不是值比较）</li></ul></li></ul><h3 id="k-means性能评估" tabindex="-1"><a class="header-anchor" href="#k-means性能评估" aria-hidden="true">#</a> k-means性能评估</h3><p>轮廓系数：<code>sc_i = (b_i − a_i) / max⁡(b_i, a_i)</code></p><blockquote><p>说明：点i为已聚类数据中的样本，b_i为点i到其它族群的所有样本之间的距离的平均值中的最小值，a_i为点i到本身所在族群的其他样本之间的距离的平均值（外部距离最大化，内部距离最小化）。最终计算出所有样本点的轮廓系数的平均值</p></blockquote><p>最终得到轮廓系数<code>sc_i</code>的范围在[-1, 1]之间，而且越靠近1说明聚类的效果越好</p><ul><li><p><strong>k-means性能评估的API</strong></p><p><code>sklearn.metrics.silhouette_score(X, labels)</code></p><blockquote><p><code>X</code>：特征值</p><p><code>labels</code>：k-means算法使用特征值计算得到的目标值</p></blockquote></li><li><p><strong>k-means总结</strong></p><p><strong>特点</strong>：采用迭代式算法，直观易懂并且非常实用</p><p><strong>缺点</strong>：容易收敛到局部最优解（使用多次聚类进行解决）</p><p><em><strong>注</strong></em>：聚类一般做在分类之前</p></li></ul>',46),p=[n];function l(a,d){return o(),t("div",null,p)}const g=e(s,[["render",l],["__file","03_回归算法与非监督学习.html.vue"]]);export{g as default};
