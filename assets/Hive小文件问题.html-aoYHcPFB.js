import{_ as e}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as p,o as t,c as o,a as c,b as n,d as s,e as l,f as i}from"./app-VKLC1A-x.js";const r={},u=n("p",null,"分析 Hive 小文件产生的原因以及影响，并提出解决方案",-1),d=n("h1",{id:"hive-小文件问题",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#hive-小文件问题","aria-hidden":"true"},"#"),s(" Hive 小文件问题")],-1),k={href:"https://mp.weixin.qq.com/s/vXpktfjcAxxe0_d-5Y-xjg",target:"_blank",rel:"noopener noreferrer"},m=i(`<h2 id="小文件产生的原因" tabindex="-1"><a class="header-anchor" href="#小文件产生的原因" aria-hidden="true">#</a> 小文件产生的原因</h2><h3 id="_1-直接插入数据" tabindex="-1"><a class="header-anchor" href="#_1-直接插入数据" aria-hidden="true">#</a> 1.直接插入数据</h3><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> A <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">&#39;zhangsan&#39;</span><span class="token punctuation">,</span> <span class="token number">88</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">&#39;lisi&#39;</span><span class="token punctuation">,</span> <span class="token number">61</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这种方式每次插入时都会产生一个文件，多次插入少量数据就会出现多个小文件，但是这种方式生产环境很少使用，可以说基本没有使用的</p><h3 id="_2-load-加载数据" tabindex="-1"><a class="header-anchor" href="#_2-load-加载数据" aria-hidden="true">#</a> 2.load 加载数据</h3><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">-- 导入文件</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">&#39;/export/score.csv&#39;</span> overwrite <span class="token keyword">into</span> <span class="token keyword">table</span> A<span class="token punctuation">;</span>

<span class="token comment">-- 导入文件夹</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">&#39;/export/score&#39;</span> overwrite <span class="token keyword">into</span> <span class="token keyword">table</span> A<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>使用 load 方式可以导入文件或文件夹，当导入一个文件时，Hive 表就有一个文件，当导入文件夹时，Hive 表的文件数量和文件夹下文件的数量相同，即导入的文件越多，Hive 表的文件数量也越多</p><h3 id="_3-通过查询加载数据" tabindex="-1"><a class="header-anchor" href="#_3-通过查询加载数据" aria-hidden="true">#</a> 3.通过查询加载数据</h3><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> A <span class="token keyword">select</span> s_id<span class="token punctuation">,</span>c_name<span class="token punctuation">,</span> s_score <span class="token keyword">from</span> B<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这种方式是生产环境中常用的，也是最容易产生小文件的方式</p><p>insert 导入数据时会启动 MapReduce 任务，MapReduce 中 Reduce 有多少个就输出多少个文件，所以，文件数量 = ReduceTask 数量 * 分区数</p><p>也有很多简单任务没有 Reduce，只有 Map 阶段，则，文件数量 = MapTask 数量 * 分区数</p><p>每执行一次 insert 时 Hive 中至少产生一个文件，因为 insert 导入时至少会有一个 MapTask，像有的业务需要每 10 分钟就要把数据同步到 Hive 中，这样产生的文件就会很多</p><h2 id="小文件过多的影响" tabindex="-1"><a class="header-anchor" href="#小文件过多的影响" aria-hidden="true">#</a> 小文件过多的影响</h2><ol><li><p>首先对底层存储 HDFS 来说，HDFS 本身就不适合存储大量小文件，小文件过多会导致 NameNode 元数据特别大，占用太多内存，严重影响 HDFS 的性能</p></li><li><p>对 Hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个 Map 任务来完成，而一个 Map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 Map 数量是受限的，会导致任务排除阻塞</p></li></ol><h2 id="解决小文件过多的方案" tabindex="-1"><a class="header-anchor" href="#解决小文件过多的方案" aria-hidden="true">#</a> 解决小文件过多的方案</h2><h3 id="_1-concatenate-命令" tabindex="-1"><a class="header-anchor" href="#_1-concatenate-命令" aria-hidden="true">#</a> 1.concatenate 命令</h3><p>使用 Hive 自带的 concatenate 命令，自动合并小文件</p><p>使用方法：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">-- 对于非分区表</span>
<span class="token keyword">alter</span> <span class="token keyword">table</span> A concatenate<span class="token punctuation">;</span>

<span class="token comment">-- 对于分区表</span>
<span class="token keyword">alter</span> <span class="token keyword">table</span> B <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token number">20201224</span><span class="token punctuation">)</span> concatenate<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>示例：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 向 A 表中插入数据</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">&gt;</span> insert into table A values <span class="token punctuation">(</span><span class="token number">1</span>, <span class="token string">&#39;aa&#39;</span>, <span class="token number">67</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token number">2</span>, <span class="token string">&#39;bb&#39;</span>, <span class="token number">87</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">&gt;</span> insert into table A values <span class="token punctuation">(</span><span class="token number">3</span>, <span class="token string">&#39;cc&#39;</span>, <span class="token number">67</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token number">4</span>, <span class="token string">&#39;dd&#39;</span>, <span class="token number">87</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">&gt;</span> insert into table A values <span class="token punctuation">(</span><span class="token number">5</span>, <span class="token string">&#39;ee&#39;</span>, <span class="token number">67</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token number">6</span>, <span class="token string">&#39;ff&#39;</span>, <span class="token number">87</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment"># 执行以上三条语句，则 A 表下就会有三个小文件，在 Hive 命令行执行如下语句</span>
<span class="token comment"># 查看 A 表下文件数量</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">&gt;</span> dfs <span class="token parameter variable">-ls</span> /user/hive/warehouse/A<span class="token punctuation">;</span>
Found <span class="token number">3</span> items
-rwxr-xr-x   <span class="token number">3</span> root supergroup        <span class="token number">378</span> <span class="token number">2020</span>-12-24 <span class="token number">14</span>:46 /user/hive/warehouse/A/000000_0
-rwxr-xr-x   <span class="token number">3</span> root supergroup        <span class="token number">378</span> <span class="token number">2020</span>-12-24 <span class="token number">14</span>:47 /user/hive/warehouse/A/000000_0_copy_1
-rwxr-xr-x   <span class="token number">3</span> root supergroup        <span class="token number">378</span> <span class="token number">2020</span>-12-24 <span class="token number">14</span>:48 /user/hive/warehouse/A/000000_0_copy_2

<span class="token comment"># 可以看到有三个小文件，然后使用 concatenate 进行合并</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">&gt;</span> alter table A concatenate<span class="token punctuation">;</span>

<span class="token comment"># 再次查看 A 表下文件数量</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">&gt;</span> dfs <span class="token parameter variable">-ls</span> /user/hive/warehouse/A<span class="token punctuation">;</span>
Found <span class="token number">1</span> items
-rwxr-xr-x   <span class="token number">3</span> root supergroup        <span class="token number">778</span> <span class="token number">2020</span>-12-24 <span class="token number">14</span>:59 /user/hive/warehouse/A/000000_0

<span class="token comment"># 已合并成一个文件</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p><strong>注意</strong>：</p><p><em><strong>1&gt;</strong></em> <code>concatenate</code> 命令只支持 RCFILE 和 ORC 文件类型</p><p><em><strong>2&gt;</strong></em> 使用 <code>concatenate</code> 命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令</p><p><em><strong>3&gt;</strong></em> 当多次使用 <code>concatenate</code> 后文件数量不再变化，这个跟参数 <code>mapreduce.input.fileinputformat.split.minsize=256mb</code> 的设置有关，可设定每个文件的最小 size</p></blockquote><h3 id="_2-减少-map-数量" tabindex="-1"><a class="header-anchor" href="#_2-减少-map-数量" aria-hidden="true">#</a> 2.减少 Map 数量</h3><ul><li><p><strong>设置 Map 输入合并小文件的相关参数</strong>：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">-- 执行 Map 前进行小文件合并</span>
<span class="token comment">-- CombineHiveInputFormat 底层是 Hadoop 的 CombineFileInputFormat 方法</span>
<span class="token comment">-- 此方法是在 Map 阶段将多个文件合成一个 split 作为输入</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>CombineHiveInputFormat<span class="token punctuation">;</span>  <span class="token comment">-- 默认</span>

<span class="token comment">-- 每个 Map 最大输入大小（这个值决定了合并后文件的数量）</span>
<span class="token keyword">set</span> mapred<span class="token punctuation">.</span>max<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token number">256000000</span><span class="token punctuation">;</span>  <span class="token comment">-- 256M</span>

<span class="token comment">-- 一个节点上 split 的至少的大小（这个值决定了多个 DataNode 上的文件是否需要合并）</span>
<span class="token keyword">set</span> mapred<span class="token punctuation">.</span>min<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>node<span class="token operator">=</span><span class="token number">100000000</span><span class="token punctuation">;</span>  <span class="token comment">-- 100M</span>

<span class="token comment">-- 一个交换机下 split 的至少的大小（这个值决定了多个交换机上的文件是否需要合并）</span>
<span class="token keyword">set</span> mapred<span class="token punctuation">.</span>min<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>rack<span class="token operator">=</span><span class="token number">100000000</span><span class="token punctuation">;</span>  <span class="token comment">-- 100M</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p><strong>设置 Map 输出和 Reduce 输出进行合并的相关参数</strong>：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">-- 设置 Map 端输出进行合并，默认为 true</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapfiles <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">-- 设置 Reduce 端输出进行合并，默认为 false</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapredfiles <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">-- 设置合并文件的大小</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>task <span class="token operator">=</span> <span class="token number">256</span><span class="token operator">*</span><span class="token number">1000</span><span class="token operator">*</span><span class="token number">1000</span><span class="token punctuation">;</span>  <span class="token comment">-- 256M</span>

<span class="token comment">-- 当输出文件的平均大小小于该值时，启动一个独立的 MapReduce 任务进行文件 merge</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>smallfiles<span class="token punctuation">.</span>avgsize<span class="token operator">=</span><span class="token number">16000000</span><span class="token punctuation">;</span>  <span class="token comment">-- 16M</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p><strong>启用压缩</strong>：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">-- hive 的查询结果输出是否进行压缩</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>compress<span class="token punctuation">.</span>output<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">-- MapReduce Job 的结果输出是否使用压缩</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h3 id="_3-减少-reduce-数量" tabindex="-1"><a class="header-anchor" href="#_3-减少-reduce-数量" aria-hidden="true">#</a> 3.减少 Reduce 数量</h3><p>Reduce 的个数决定了输出文件的个数，所以可以通过调整 Reduce 的个数控制 Hive 表的文件数量，Hive 中的分区函数 <code>distribute by</code> 正好是控制 MapReduce 中 partition 分区的，然后通过设置 Reduce 的数量，结合分区函数让数据均衡的进入每个 Reduce 即可</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">-- 设置 Reduce 的数量有两种方式，第一种是直接设置 Reduce 个数</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">;</span>

<span class="token comment">-- 第二种是设置每个 Reduce 的大小，Hive 会根据数据总大小猜测确定一个 Reduce 个数</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>bytes<span class="token punctuation">.</span>per<span class="token punctuation">.</span>reducer<span class="token operator">=</span><span class="token number">5120000000</span><span class="token punctuation">;</span>  <span class="token comment">-- 默认是 1G，设置为 5G</span>

<span class="token comment">-- 执行以下语句，将数据均衡的分配到 Reduce 中</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">;</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> A <span class="token keyword">partition</span><span class="token punctuation">(</span>dt<span class="token punctuation">)</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> B
distribute <span class="token keyword">by</span> rand<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">-- 解释：如设置 Reduce 数量为 10，则使用 rand()，随机生成一个数 x % 10，</span>
<span class="token comment">-- 这样数据就会随机进入 Reduce 中，防止出现有的文件过大或过小</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-archive-小文件归档" tabindex="-1"><a class="header-anchor" href="#_4-archive-小文件归档" aria-hidden="true">#</a> 4.Archive 小文件归档</h3><p>Hadoop Archive 简称 HAR，是一个高效地将小文件放入 HDFS 块中的文件存档工具，它能够将多个小文件打包成一个 HAR 文件，这样在减少 NameNode 内存使用的同时，仍然允许对文件进行透明的访问</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">-- 用来控制归档是否可用</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>archive<span class="token punctuation">.</span>enabled<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">-- 通知 Hive 在创建归档时是否可以设置父目录</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>archive<span class="token punctuation">.</span>har<span class="token punctuation">.</span>parentdir<span class="token punctuation">.</span>settable<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">-- 控制需要归档文件的大小</span>
<span class="token keyword">set</span> har<span class="token punctuation">.</span>partfile<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token number">1099511627776</span><span class="token punctuation">;</span>

<span class="token comment">-- 使用以下命令进行归档</span>
<span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> A ARCHIVE <span class="token keyword">PARTITION</span><span class="token punctuation">(</span>dt<span class="token operator">=</span><span class="token string">&#39;2020-12-24&#39;</span><span class="token punctuation">,</span> hr<span class="token operator">=</span><span class="token string">&#39;12&#39;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">-- 对已归档的分区恢复为原文件</span>
<span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> A UNARCHIVE <span class="token keyword">PARTITION</span><span class="token punctuation">(</span>dt<span class="token operator">=</span><span class="token string">&#39;2020-12-24&#39;</span><span class="token punctuation">,</span> hr<span class="token operator">=</span><span class="token string">&#39;12&#39;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p><strong>注意</strong>：</p><p><em><strong>归档的分区可以查看，但是不能 insert overwrite，必须先 unarchive</strong></em></p></blockquote><h2 id="小建议" tabindex="-1"><a class="header-anchor" href="#小建议" aria-hidden="true">#</a> 小建议</h2><p>如果是新集群，没有历史遗留问题的话，建议 Hive 使用 ORC 文件格式，以及启用 LZO 压缩，这样小文件过多可以使用 Hive 自带命令 <code>concatenate</code> 快速合并</p>`,34);function v(b,h){const a=p("ExternalLinkIcon");return t(),o("div",null,[u,c(" more "),d,n("p",null,[s("参考资料："),n("a",k,[s("如何有效解决hive小文件过多问题？"),l(a)])]),m])}const y=e(r,[["render",v],["__file","Hive小文件问题.html.vue"]]);export{y as default};
