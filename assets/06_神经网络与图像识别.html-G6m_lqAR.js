import{_ as n}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as e,c as t,f as a}from"./app-VKLC1A-x.js";const o="/mynotes/assets/卷积神经网络的计算-QEl_F78X.png",s={},p=a(`<h1 id="神经网络与图像识别" tabindex="-1"><a class="header-anchor" href="#神经网络与图像识别" aria-hidden="true">#</a> 神经网络与图像识别</h1><h2 id="神经网络介绍" tabindex="-1"><a class="header-anchor" href="#神经网络介绍" aria-hidden="true">#</a> 神经网络介绍</h2><p><strong>定义</strong>：在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络的结构和功能的计算模型，用于对函数进行估计或近似</p><ul><li><p><strong>神经网络的种类</strong></p><p><strong>基础神经网络</strong>：单层感知器，线性神经网络，BP神经网络，Hopfield神经网络等</p><p><strong>进阶神经网络</strong>：玻尔兹曼机，受限玻尔兹曼机，递归神经网络等</p><p><strong>深度神经网络</strong>：深度置信网络，卷积神经网络，循环神经网络，LSTM网络等</p></li><li><p><strong>神经网络的特点</strong></p><p>输入向量的维度和输入神经元的个数相同</p><p>每个连接都有个权值</p><p>同一层神经元之间没有连接</p><p>由输入层，隐层，输出层组成</p><p>第N层与第N-1层的所有神经元连接，也叫<em><strong>全连接</strong></em></p></li><li><p><strong>神经网络的组成</strong></p><p><strong>结构</strong>（Architecture）：例如，神经网络中的变量可以是神经元连接的权重</p><p><strong>激励函数</strong>（Activity Rule）：大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值</p><p><strong>学习规则</strong>（Learning Rule）：学习规则指定了网络中的权重如何随着时间推进而调整（反向传播算法）</p></li><li><p><strong>神经网络的API模块</strong></p><p>在使用tensorflow的时候，tf.nn、tf.layers、tf.contrib模块有很多功能是重复的</p><ul><li><p><strong>tf.nn</strong>：提供神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluation</p></li><li><p><strong>tf.layers</strong>：主要提供高层的神经网络，主要和卷积相关的，对tf.nn的进一步封装</p></li><li><p><strong>tf.contrib</strong>：tf.contrib.layers提供够将计算图中的网络层、正则化、摘要操作、是构建计算图的高级操作，但是tf.contrib包不稳定以及有一些实验代码</p></li></ul></li></ul><h2 id="浅层神经网络" tabindex="-1"><a class="header-anchor" href="#浅层神经网络" aria-hidden="true">#</a> 浅层神经网络</h2><h3 id="特征加权的计算" tabindex="-1"><a class="header-anchor" href="#特征加权的计算" aria-hidden="true">#</a> 特征加权的计算</h3><p><code>tf.matmul(a, b, name=None) + bias</code></p><blockquote><p><strong>返回值</strong>：全连接的结果，供交叉损失运算</p></blockquote><h3 id="softmax交叉熵损失计算" tabindex="-1"><a class="header-anchor" href="#softmax交叉熵损失计算" aria-hidden="true">#</a> SoftMax交叉熵损失计算</h3><p><code>tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None, name=None)</code></p><blockquote><p>计算logits和labels之间的交叉熵损失</p><p><code>labels</code>：标签值（真实值）</p><p><code>logits</code>：样本加权之后的值（预测值）</p><p><strong>返回值</strong>：返回损失值列表</p></blockquote><p><code>tf.reduce_mean(input_tensor)</code>：计算损失值列表的平均值</p><h3 id="梯度下降优化" tabindex="-1"><a class="header-anchor" href="#梯度下降优化" aria-hidden="true">#</a> 梯度下降优化</h3><p><code>tf.train.GradientDescentOptimizer(learning_rate)</code></p><blockquote><p>参数：<code>learning_rate</code>：学习率</p><p>方法：<code>minimize(loss)</code>：最小化损失</p><p><strong>返回值</strong>：梯度下降op</p></blockquote><h3 id="计算准确率" tabindex="-1"><a class="header-anchor" href="#计算准确率" aria-hidden="true">#</a> 计算准确率</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>equal_list <span class="token operator">=</span> tf<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_predict<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>equal_list<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="卷积神经网络" tabindex="-1"><a class="header-anchor" href="#卷积神经网络" aria-hidden="true">#</a> 卷积神经网络</h2><p><strong>深层神经网络</strong>与常见的单一隐藏层神经网络的区别在于深度，深度学习网络中，每一个节点层在前一层输出的基础上学习识别一组特定的特征。随着神经网络深度增加，节点所能识别的特征也就越来越复杂</p><p>神经网络的基本组成包括输入层、隐藏层、输出层；而<strong>卷积神经网络</strong>的特点在于<em><strong>隐藏层分为卷积层和池化层</strong></em>（pooling layer，又叫下采样层）</p><p><strong>卷积层</strong>：通过在原始图像上平移来提取特征，每一个特征就是一个特征映射</p><p><strong>池化层</strong>：通过特征后稀疏参数来减少学习的参数，降低网络的复杂度（最大池化和平均池化）</p><p>卷积神经网络的<strong>结构</strong>包括：卷积层过滤器、激活函数、池化层、全连接层</p><h3 id="卷积层" tabindex="-1"><a class="header-anchor" href="#卷积层" aria-hidden="true">#</a> 卷积层</h3><ul><li><p><strong>卷积层的零填充</strong></p><p>卷积核在提取特征映射时的动作称之为padding（零填充），由于移动步长不一定能整除整张图的像素宽度，对此有两种处理方式：SAME和VALID</p><p><strong>SAME</strong>：越过边缘取样，取样的面积和输入图像的像素宽度一致</p><p><strong>VALID</strong>：不越过边缘取样，取样的面积小于输入图像的像素宽度</p></li><li><p><strong>卷积层的计算</strong></p><img src="`+o+'" alt="卷积神经网络的计算" style="zoom:40%;"></li><li><p><strong>卷积层的API</strong></p><p><code>tf.nn.conv2d(input, filter, strides, padding, name=None)</code></p><blockquote><p>计算给定4-D的input和2-D的filter张量的二维卷积</p><p><code>input</code>：输入的4-D张量，[batch, heigth, width, channel]，类型为float32/64</p><p><code>filter</code>：指定过滤器的大小（一般为1×1、3×3或5×5），同样为4-D，[filter_height, filter_width, in_channels, out_channels]</p><p><code>strides</code>：指定的步长，一般设置为1，[1, stride, stride, 1]</p><p><code>padding</code>：零填充的方式，有<code>SAME</code>和<code>VALID</code>可选</p></blockquote></li></ul><h3 id="激活函数" tabindex="-1"><a class="header-anchor" href="#激活函数" aria-hidden="true">#</a> 激活函数</h3><p><code>tf.nn.relu(features, name=None)</code></p><blockquote><p><code>features</code>：卷积后加上偏置的结果</p></blockquote><ul><li><p><strong>和sigmoid的比较</strong>：</p><p><em><strong>1&gt;</strong></em> 采用sigmoid等函数，反向传播求误差梯度时，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多</p><p><em><strong>2&gt;</strong></em> 对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（求不出权重和偏置）</p></li></ul><h3 id="池化层" tabindex="-1"><a class="header-anchor" href="#池化层" aria-hidden="true">#</a> 池化层</h3><p>池化层（Pooling）的主要作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数的数量。Pooling的方法有很多，最常用的是Max Pooling（大小一般为2×2，步长为2）</p><ul><li><p><strong>池化层的API</strong></p><p><code>tf.nn.max_pool(value, ksize, strides, padding, name=None)</code></p><blockquote><p><code>value</code>：4-D的张量，[batch, height, width, channels]，类型为float32</p><p><code>ksize</code>：池化窗口的大小，[1, ksize, ksize, 1]</p><p><code>strides</code>：步长，[1, strides, strides, 1]</p><p><code>padding</code>：零填充的方式，有<code>SAME</code>和<code>VALID</code>可选</p></blockquote></li></ul><h3 id="全连接层" tabindex="-1"><a class="header-anchor" href="#全连接层" aria-hidden="true">#</a> 全连接层</h3><p>前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。最后的全连接层在整个卷积神经网络中起到“分类器”的作用</p>',34),r=[p];function i(c,d){return e(),t("div",null,r)}const g=n(s,[["render",i],["__file","06_神经网络与图像识别.html.vue"]]);export{g as default};
